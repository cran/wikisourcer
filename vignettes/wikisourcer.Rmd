---
title: "Introduction to wikisourcer"
author: "Félix Luginbül"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to wikisourcer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6)
```

The digital library [Wikisource](https://wikisource.org/), a sister projet of *Wikipedia*, hosts books in the public domain in almost all languages. More than 100'000 books are accessible in English, Spanish, French, German, Russian or Chinese.

The **wikisourcer** R package helps you download any book or page from Wikisource. The text is downloaded in a tidy data frame, so it can be analyzed within the [tidyverse](https://www.tidyverse.org/) ecosystem.

## Download books

To download Voltaire's philosophical novel *Candide*, simply paste the url of the table of content into the ```wikisource_book``` function. Note that the book is already classified by chapter with the ```page``` variable.

```{r}
library(wikisourcer)

wikisource_book("https://en.wikisource.org/wiki/Candide")
```

Multiple books can easily be downoaded using the ```purrr``` package. For example, we can download *Candide* in French, English, Spanish and Italian.

```{r}
library(purrr)

fr <- "https://fr.wikisource.org/wiki/Candide,_ou_l%E2%80%99Optimisme/Garnier_1877"
en <- "https://en.wikisource.org/wiki/Candide"
es <- "https://es.wikisource.org/wiki/C%C3%A1ndido,_o_el_optimismo"
it <- "https://it.wikisource.org/wiki/Candido"
urls <- c(fr, en, es, it)

candide <- purrr::map_df(urls, wikisource_book)
```

Before making a text analysis, the text should be cleaned from remaining Wikisource metadata.

```{r message=FALSE}
library(stringr)
library(dplyr)

candide_cleaned <- candide %>%
  filter(!str_detect(text, "CHAPITRE|↑")) %>% #clean French
  filter(!str_detect(text, "CAPITULO")) %>% #clean Spanish
  filter(!str_detect(text, "../|IncludiIntestazione|Romanzi|^\\d+")) #clean Italian
```

We can now compare the number of words in each chapter by language.

```{r text_analysis}
library(tidytext)
library(ggplot2)

candide_cleaned %>%
  tidytext::unnest_tokens(word, text) %>%
  count(page, language, sort = TRUE) %>%
  ggplot(aes(x = as.factor(page), y = n, fill = language)) +
    geom_col(position = "dodge") +
    theme_minimal() +
    labs(x = "chapter", y = "number of words",
         title = "Multilingual Text analysis of Voltaire's Candide")
```

## Download pages

The ```wikisource_book``` function sometimes doesn't work. It happens when the main url path differs from the ones of the linked urls. This issue can easily be fixed using the ```wikisource_page``` function.

The ```wikisource_page``` function has two arguments, i.e. the Wikisource url and an optional title for the page. For example, we can download *Sonnet 18* of William Shakespeare.

```{r}
library(wikisourcer)

wikisource_page("https://en.wikisource.org/wiki/Sonnet_18_(Shakespeare)", "Sonnet 18")
```

Let's try to download the 154 Sonnets of William Shakespeare using ```wikisource_book```.

```{r}
wikisource_book("https://en.wikisource.org/wiki/The_Sonnets")
```

The download failed because the main wiki url ```wiki/The_Sonnets``` differs from the wiki path of the pages, i.e. ```wiki/Sonnet_```.

We have to use the ```wikisource_page``` function to download the 154 Sonnets. 

Note that the base R function ```paste0``` is very useful to create a list of urls. We will also use ```paste0``` to name the pages for the second argument of the ```wikisource_page``` function.

```{r}
urls <- paste0("https://en.wikisource.org/wiki/Sonnet_", 1:154, "_(Shakespeare)") #154 urls

sonnets <- purrr::map2_df(urls, paste0("Sonnet ", 1:154), wikisource_page)
sonnets
```

We can make a text similarity analysis. Which sonnets are the closest to each others in terms of words used?

```{r text_similarity, message=FALSE, warning=FALSE}
library(widyr)
library(SnowballC)
library(igraph)
library(ggraph)

sonnets_similarity <- sonnets %>%
  filter(!str_detect(text, "public domain|Public domain")) %>% #clean text
  tidytext::unnest_tokens(word, text) %>%
  anti_join(tidytext::get_stopwords("en")) %>%
  anti_join(data_frame(word = c("thy", "thou", "thee"))) %>% #old English stopwords
  mutate(wordStem = SnowballC::wordStem(word)) %>% #Stemming
  count(page, wordStem) %>%
  widyr::pairwise_similarity(page, wordStem, n) %>%
  filter(similarity > 0.3)

# themes by sonnet 
theme <- data_frame(page = unique(sonnets$page),
                    theme = c(rep("Procreation", times = 17), rep("Fair Youth", times = 60),
                              rep("Rival Poet", times = 9), rep("Fair Youth", times = 12),
                              rep("Irregular", times = 1), rep("Fair Youth", times = 26),
                              rep("Irregular", times = 1), rep("Dark Lady", times = 28))) %>%
  filter(page %in% sonnets_similarity$item1 |
         page %in% sonnets_similarity$item2)

set.seed(1234)

sonnets_similarity %>%
  graph_from_data_frame(vertices = theme) %>%
  ggraph() +
  geom_edge_link(aes(edge_alpha = similarity)) +
  geom_node_point(aes(color = theme), size = 3) +
  geom_node_text(aes(label = name), size = 3.5, check_overlap = TRUE, vjust = 1) +
  theme_void() +
  labs(title = "Closest Shakespeare's Sonnets to each others in terms of words used")

```

## Other wikisources

The **wikisourcer** functions work with other wiki websites. 

For example, the website [Bibliowiki](https://biblio.wiki/) hosts texts and images in the public domain under Canadian copyright law. We have therefore access to George Orwell's novel *Nineteen Eighty-Four*.

```{r echo = TRUE, results = 'hide'}
orwell <- wikisource_book("https://biblio.wiki/wiki/Nineteen_Eighty-Four")
```

Let's make a sentiment analysis of Orwell's dystopian novel *1984*.

```{r sentiment_analysis, message=FALSE}
library(tidyr)

orwell_sent <- orwell %>%
  filter(page != 25) %>% #remove appendix page
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing")) %>%
  anti_join(get_stopwords("en")) %>%
  count(page, sentiment) %>%
  spread(key = sentiment, value = n) %>%
  mutate(sentiment = positive - negative)

ggplot(orwell_sent, aes(page, sentiment)) +
  geom_col() +
  geom_smooth(method = "loess", se = FALSE) +
  scale_x_continuous(breaks = c(1:24)) +
  theme_minimal() +
  labs(title = "Sentiment Analysis of Orwell's 1984",
       subtitle = "Positive-negative words difference, by chapter",
       x = "chapter", y = "sentiment score")
```

The overall negative sentiment score reflects plainly the dark and pessimistic tone of the novel.
